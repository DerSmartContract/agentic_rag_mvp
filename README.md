# Agentic RAG MVP
Ein modulares Retrieval-Augmented-Generation-System mit Hybrid-LLM-Integration und persistentem Speicher.

---

## ProjektÃ¼bersicht

Dieses Projekt implementiert ein minimales, erweiterbares agentenbasiertes RAG-System (Retrieval-Augmented Generation), das externe Informationsquellen mit leistungsstarken Sprachmodellen kombiniert, um kontextbezogene Antworten zu generieren.

### Hauptfunktionen

- **Modulare Agentenarchitektur**: Separate Agenten fÃ¼r lokale Daten, Websuche und Cloud-Quellen
- **Persistenter Speicher**: JSON-basierter Langzeitspeicher fÃ¼r Kontexte
- **Hybridbetrieb fÃ¼r Sprachmodelle**: Umschaltbar zwischen OpenAI (z.â€¯B. GPT-4o) und lokalen Modellen (Ã¼ber LM Studio)
- **Asynchrone Verarbeitung**: Aufgaben werden parallel Ã¼ber `asyncio` ausgefÃ¼hrt
- **Konfigurierbares Verhalten Ã¼ber .env-Datei**: Einfacher Wechsel zwischen Modellen

---

## Projektstruktur

```bash
agentic_rag_mvp/
â”œâ”€â”€ main.py                                # Asynchroner Einstiegspunkt
â”œâ”€â”€ .env                                   # Konfigurationsdatei: MODE und API-Key
â”œâ”€â”€ data/memory.json                       # Persistente Kontextsicherung
â”œâ”€â”€ requirements.txt                       # AbhÃ¤ngigkeiten
â”œâ”€â”€ aggregator/aggregator.py               # Zentrale Koordination & Planung
â”œâ”€â”€ agents/                                # Subagenten (lokal, web, cloud)
â”œâ”€â”€ planner/planner.py                     # Planungsmodul zur Aufgabensteuerung
â”œâ”€â”€ memory/memory.py                       # Speicher-Engine mit Ã„hnlichkeitsabgleich
â””â”€â”€ utils/rag_model.py                     # Schnittstelle zu OpenAI oder LM Studio
```

---

## Installation

```bash
git clone https://github.com/<dein-benutzername>/agentic-rag-mvp.git
cd agentic-rag-mvp
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## Konfiguration

Erstelle eine `.env`-Datei im Projektverzeichnis:

```dotenv
MODE=LOCAL                  # oder OPENAI
OPENAI_API_KEY=sk-...       # nur erforderlich bei MODE=OPENAI
```

---

## Anwendung starten

```bash
python main.py
```

---

## Kontextspeicherung (Memory)

Das System speichert alle Anfragen und zugehÃ¶rigen Kontexte automatisch in `data/memory.json` und ruft bei Ã¤hnlichen Anfragen passende Kontexte wieder ab. So wird Redundanz vermieden und die Effizienz gesteigert.

---

## Integration mit LM Studio

1. LM Studio starten
2. Ein Modell laden (z.â€¯B. Mistral 7B)
3. API-Zugriff aktivieren (â€Enable APIâ€œ)
4. In `.env` den Modus auf `MODE=LOCAL` setzen
5. Anwendung starten

---

## ğŸ”­ Weiterentwicklung & Roadmap

Wir entwickeln das System iterativ weiter in Richtung einer agentenbasierten Plattform mit SelbstÃ¼berwachung, Planungslogik und adaptivem LangzeitgedÃ¤chtnis.

| Modul / Funktion                     | Status         | Beschreibung |
|--------------------------------------|----------------|--------------|
| ğŸ§  Vektorbasierter Speicher (ChromaDB) | In Planung     | Semantische Ã„hnlichkeit statt reinem Textvergleich |
| ğŸ§© ReAct-Agentenplanung                | Konzeptphase   | Reasoning + Acting in verketteten AgentenablÃ¤ufen |
| ğŸ’» Web-OberflÃ¤che (Next.js / Streamlit)| In Entwicklung | NutzeroberflÃ¤che fÃ¼r Queries, Logs, Memory        |
| ğŸ³ Docker / CI/CD Deployment          | Bereit         | Containerisierung & GitHub Actions                |
| ğŸ” Authentifizierungsmodul (JWT)      | Geplant        | Sichere API-Nutzung & Mehrbenutzerbetrieb         |
| ğŸ§ª Unit-Tests & Coverage              | In Entwicklung | Testabdeckung fÃ¼r Aggregator, Memory & Agents     |
| ğŸ“Š Metrik-Logging & Tracing           | Implementiert  | Logging aller Agentenentscheidungen und Antworten |
---
